{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liangchingyun/A-Convolution-Module-For-Variable-Input-Channels/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR7mAkCyFIUK",
        "outputId": "9a8dc89b-e7cb-4ba1-928a-8b9a706b1a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'A-Convolution-Module-For-Variable-Input-Channels'...\n",
            "remote: Enumerating objects: 64293, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 64293 (delta 10), reused 5 (delta 1), pack-reused 64272\u001b[K\n",
            "Receiving objects: 100% (64293/64293), 1.35 GiB | 29.30 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "Updating files: 100% (64229/64229), done.\n",
            "/content/A-Convolution-Module-For-Variable-Input-Channels\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/liangchingyun/A-Convolution-Module-For-Variable-Input-Channels.git\n",
        "%cd A-Convolution-Module-For-Variable-Input-Channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hnvhtd-v6u3n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import argparse\n",
        "# from .utils import load_state_dict_from_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9PJTFJ_0ull"
      },
      "outputs": [],
      "source": [
        "class attention2d(nn.Module):\n",
        "    def __init__(self, in_planes, ratios, K, temperature, init_weight=True):\n",
        "        super(attention2d, self).__init__()\n",
        "        assert temperature%3==1\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        if in_planes!=3:\n",
        "            hidden_planes = int(in_planes*ratios)+1\n",
        "        else:\n",
        "            hidden_planes = K\n",
        "        self.fc1 = nn.Conv2d(in_planes, hidden_planes, 1, bias=False)\n",
        "        # self.bn = nn.BatchNorm2d(hidden_planes)\n",
        "        self.fc2 = nn.Conv2d(hidden_planes, K, 1, bias=True)\n",
        "        self.temperature = temperature\n",
        "        if init_weight:\n",
        "            self._initialize_weights()\n",
        "\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            if isinstance(m ,nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def updata_temperature(self):\n",
        "        if self.temperature!=1:\n",
        "            self.temperature -=3\n",
        "            print('Change temperature to:', str(self.temperature))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avgpool(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x).view(x.size(0), -1)\n",
        "        return F.softmax(x/self.temperature, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTzKdrTZAMED"
      },
      "outputs": [],
      "source": [
        "class Dynamic_conv2d(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, ratio=0.25, stride=1, padding=0, dilation=1, groups=1, bias=True, K=4,temperature=34, init_weight=True):\n",
        "        super(Dynamic_conv2d, self).__init__()\n",
        "        assert in_planes%groups==0\n",
        "        self.in_planes = in_planes\n",
        "        self.out_planes = out_planes\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.groups = groups\n",
        "        self.bias = bias\n",
        "        self.K = K\n",
        "        self.attention = attention2d(in_planes, ratio, K, temperature)\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(K, out_planes, in_planes//groups, kernel_size, kernel_size), requires_grad=True)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(K, out_planes))\n",
        "        else:\n",
        "            self.bias = None\n",
        "        if init_weight:\n",
        "            self._initialize_weights()\n",
        "\n",
        "        #TODO 初始化\n",
        "    def _initialize_weights(self):\n",
        "        for i in range(self.K):\n",
        "            nn.init.kaiming_uniform_(self.weight[i])\n",
        "\n",
        "\n",
        "    def update_temperature(self):\n",
        "        self.attention.updata_temperature()\n",
        "\n",
        "    def forward(self, x):#将batch视作维度变量，进行组卷积，因为组卷积的权重是不同的，动态卷积的权重也是不同的\n",
        "        softmax_attention = self.attention(x)\n",
        "        batch_size, in_planes, height, width = x.size()\n",
        "        x = x.view(1, -1, height, width)# 变化成一个维度进行组卷积\n",
        "        weight = self.weight.view(self.K, -1)\n",
        "\n",
        "        # 动态卷积的权重的生成， 生成的是batch_size个卷积参数（每个参数不同）\n",
        "        aggregate_weight = torch.mm(softmax_attention, weight).view(batch_size*self.out_planes, self.in_planes//self.groups, self.kernel_size, self.kernel_size)\n",
        "        if self.bias is not None:\n",
        "            aggregate_bias = torch.mm(softmax_attention, self.bias).view(-1)\n",
        "            output = F.conv2d(x, weight=aggregate_weight, bias=aggregate_bias, stride=self.stride, padding=self.padding,\n",
        "                              dilation=self.dilation, groups=self.groups*batch_size)\n",
        "        else:\n",
        "            output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding,\n",
        "                              dilation=self.dilation, groups=self.groups * batch_size)\n",
        "\n",
        "        output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIaYozpvDKHZ"
      },
      "source": [
        "DY-ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHkarCY7DZh6"
      },
      "outputs": [],
      "source": [
        "\n",
        "__all__ = ['ResNet', 'resnet18']\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    return Dynamic_conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return Dynamic_conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False,)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "\n",
        "    def update_temperature(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Dynamic_conv2d):\n",
        "                m.update_temperature()\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4h4gxdwr6cX"
      },
      "source": [
        "Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WrgGCk5r-t0"
      },
      "outputs": [],
      "source": [
        "## Data Processing\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# load dataset\n",
        "train_dataset = torchvision.datasets.ImageFolder(root='train', transform=transform)\n",
        "val_dataset = torchvision.datasets.ImageFolder(root='val', transform=transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder(root='test', transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "D_3tOpo0OcVm",
        "outputId": "d069c484-84ba-4403-f874-64841da9189e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-771fd8721c54>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "parser = argparse.ArgumentParser(description='dynamic convolution')\n",
        "parser.add_argument('--dataset', type=str, default='ImageNet-mini', help='training dataset')\n",
        "parser.add_argument('--batch-size', type=int, default=128)\n",
        "parser.add_argument('--test-batch-size', type=int, default=20)\n",
        "parser.add_argument('--epochs', type=int, default=160)\n",
        "parser.add_argument('--lr', type=float, default=0.1, )\n",
        "parser.add_argument('--momentum', type=float, default=0.9)\n",
        "parser.add_argument('--weight-decay', type=float, default=1e-4)\n",
        "\n",
        "args = parser.parse_args()\n",
        "args.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "if args.dataset == 'ImageNet-mini':\n",
        "    numclasses=50\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.Pad(4),\n",
        "                              transforms.RandomCrop(32),\n",
        "                              transforms.RandomHorizontalFlip(),\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))\n",
        "                          ]))\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                                           transform=transforms.Compose([\n",
        "                                               transforms.ToTensor(),\n",
        "                                               transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "                                           ]))\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "model = dy_resnet18(num_classes=numclasses)\n",
        "\n",
        "model.to(args.device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "print(str(args))\n",
        "\n",
        "def adjust_lr(optimizer, epoch):\n",
        "    if epoch in [args.epochs*0.5, args.epochs*0.75, args.epochs*0.85]:\n",
        "        for p in optimizer.param_groups:\n",
        "            p['lr'] *= 0.1\n",
        "            lr = p['lr']\n",
        "        print('Change lr:'+str(lr))\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    avg_loss = 0.\n",
        "    train_acc = 0.\n",
        "    adjust_lr(optimizer, epoch)\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "\n",
        "        data, target = data.to(args.device), target.to(args.device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        avg_loss += loss.item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        train_acc += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "    print('Train Epoch: {}, loss{:.6f}, acc{}'.format(epoch, loss.item(), train_acc/len(trainloader.dataset)), end='')\n",
        "    if args.net_name.startswith('dy'):\n",
        "        model.update_temperature()\n",
        "\n",
        "\n",
        "def val(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0.\n",
        "    correct=0.\n",
        "    with torch.no_grad():\n",
        "        for data, label in testloader:\n",
        "            data, label = data.to(args.device), label.to(args.device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, label, size_average=False).item()\n",
        "            pred =  output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
        "    test_loss/=len(testloader.dataset)\n",
        "    correct = int(correct)\n",
        "    print('Test set:average loss: {:.4f}, accuracy{}'.format(test_loss, 100.*correct/len(testloader.dataset)))\n",
        "    return correct/len(testloader.dataset)\n",
        "\n",
        "\n",
        "best_val_acc=0.\n",
        "for i in range(args.epochs):\n",
        "    train(i+1)\n",
        "    temp_acc = val(i+1)\n",
        "    if temp_acc>best_val_acc:\n",
        "        best_val_acc = temp_acc\n",
        "print('Best acc{}'.format(best_val_acc))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
